{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOy9I3utkIc0RmPSK9QBo38",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "28a9b3618ced49aea406929e2a86da46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_72974ed711224275a690e173c4cfa9a9",
              "IPY_MODEL_aede58571f9e4fcb89bc7f43c271c1ef",
              "IPY_MODEL_dc7f25d3a12141289a8ae8d3c4eb40ff"
            ],
            "layout": "IPY_MODEL_07f3ad40201944beb06bd4282fc37a7a"
          }
        },
        "72974ed711224275a690e173c4cfa9a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e420afe2a8c04871950861717ba3614d",
            "placeholder": "​",
            "style": "IPY_MODEL_a15a0f557f464e6ab334d0bf01a2d144",
            "value": "Extracting Features: 100%"
          }
        },
        "aede58571f9e4fcb89bc7f43c271c1ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_723a44e8207843cea4ea51b42e321d96",
            "max": 158915,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b3d448b0761943a98c330e9bbe71a87f",
            "value": 158915
          }
        },
        "dc7f25d3a12141289a8ae8d3c4eb40ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29690ac61a584327b0bb335ba9824102",
            "placeholder": "​",
            "style": "IPY_MODEL_81277823aba3483aae5e8a83e9ef51c0",
            "value": " 158915/158915 [1:04:50&lt;00:00, 42.58image/s]"
          }
        },
        "07f3ad40201944beb06bd4282fc37a7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e420afe2a8c04871950861717ba3614d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a15a0f557f464e6ab334d0bf01a2d144": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "723a44e8207843cea4ea51b42e321d96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3d448b0761943a98c330e9bbe71a87f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "29690ac61a584327b0bb335ba9824102": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81277823aba3483aae5e8a83e9ef51c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YugalMishra/Image-Captioning/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SbIbUXLgyTVm",
        "outputId": "9f153836-e262-402d-9955-55299e37dd3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow pillow tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Kaggle directory and move the API token\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download the Flickr30k dataset\n",
        "!kaggle datasets download -d adityajn105/flickr30k\n",
        "\n",
        "# Extract the dataset\n",
        "!unzip flickr30k.zip -d ./flickr30k"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdR1EVxTzGi5",
        "outputId": "82d19670-39bc-47bf-da85-949cd9045b73"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/adityajn105/flickr30k\n",
            "License(s): CC0-1.0\n",
            "^C\n",
            "unzip:  cannot find or open flickr30k.zip, flickr30k.zip.zip or flickr30k.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JP9hqaI0p6R",
        "outputId": "6306474d-ea1c-47ca-9111-b3fbf801e7fc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kaggle.json  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "Y5ibcXDG0yhQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNhGUxA600Xj",
        "outputId": "c2db3719-e980-4276-e4ea-684cdba8c672"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ref                                                            title                                                     size  lastUpdated                 downloadCount  voteCount  usabilityRating  \n",
            "-------------------------------------------------------------  --------------------------------------------------  ----------  --------------------------  -------------  ---------  ---------------  \n",
            "atharvasoundankar/chocolate-sales                              Chocolate Sales Data 📊🍫                                  14473  2025-03-19 03:51:40.270000          10130        177  1.0              \n",
            "abdulmalik1518/mobiles-dataset-2025                            Mobiles Dataset (2025)                                   20314  2025-02-18 06:50:24.370000          16309        279  1.0              \n",
            "adilshamim8/student-depression-dataset                         Student Depression Dataset                              467020  2025-03-13 03:12:30.423000           2846         53  1.0              \n",
            "atharvasoundankar/global-cybersecurity-threats-2015-2024       🌐 Global Cybersecurity Threats (2015-2024)               48178  2025-03-16 04:23:13.343000            915         26  1.0              \n",
            "mahmoudelhemaly/students-grading-dataset                       Student Performance & Behavior Dataset                  520428  2025-02-17 17:38:46.653000          11564        182  1.0              \n",
            "bhargavchirumamilla/netflix-movies-and-tv-shows-till-2025      Netflix Movies and TV shows till 2025                  6471169  2025-03-04 01:06:12.240000           1851         30  1.0              \n",
            "amanrajput16/used-car-price-data-from-cars24                   Used Cars Market Analysis:Scraped Data from Cars24       22591  2025-03-08 11:40:03.327000           1134         23  1.0              \n",
            "abdulmoiz12/amazon-stock-data-2025                             Amazon Stock Data 2025                                  160519  2025-03-01 09:08:07.890000           1451         29  1.0              \n",
            "ricgomes/global-fashion-retail-stores-dataset                  Global Fashion Retail Sales                          234910599  2025-03-19 18:37:15.857000           1681         28  1.0              \n",
            "ankushpanday2/heart-attack-prediction-in-indonesia             Heart Attack Prediction in Indonesia                   5398776  2025-03-11 15:19:28.123000            838         25  1.0              \n",
            "zahidmughal2343/employee-data                                  Employee Data                                           379143  2025-03-08 19:36:42.953000           1329         26  1.0              \n",
            "shohinurpervezshohan/freelancer-earnings-and-job-trends        Freelancer Earnings & Job Trends                         52906  2025-03-08 07:21:46.633000           1317         29  0.9411765        \n",
            "smayanj/netflix-users-database                                 Netflix Users Database                                  362559  2025-03-08 12:08:09.403000           2210         50  1.0              \n",
            "atharvasoundankar/global-food-wastage-dataset-2018-2024        🌍 Global Food Wastage Dataset (2018-2024) 🍽️            108611  2025-03-12 04:51:27.083000           1380         26  1.0              \n",
            "aradhanahirapara/income-survey-finance-analysis                Income Survey | Finance Analysis                       1826775  2025-03-20 19:53:01.633000           1043         24  0.7647059        \n",
            "willianoliveiragibin/grocery-inventory                         Grocery Inventory                                        50801  2025-03-16 21:03:55.307000           1010         28  1.0              \n",
            "mahatiratusher/flight-price-dataset-of-bangladesh              Flight Price Dataset of Bangladesh                     3506315  2025-03-04 04:08:49.600000           1051         23  0.9411765        \n",
            "aniruddhawankhede/mental-heath-analysis-among-teenagers        Mental_Heath_Analysis_Among_Teenagers                   177089  2025-03-10 12:17:22.827000           1935         34  1.0              \n",
            "maicolab/university-admission                                  Student performance on the entrance exam                376010  2025-03-02 17:13:46.457000            776         23  1.0              \n",
            "parsabahramsari/wdi-education-health-and-employment-2011-2021  WDI: Education, Health & Employment (2011-2021)         136185  2025-03-07 08:29:38.617000           1143         30  1.0              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d adityajn105/flickr30k"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeEpLdGD06Og",
        "outputId": "8ad3c503-9e8a-4c2a-d7cd-b3f5603f7e20"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/adityajn105/flickr30k\n",
            "License(s): CC0-1.0\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d adityajn105/flickr30k --force"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x76a_bLz1Fqs",
        "outputId": "ebe2b3f1-b52e-414d-8cea-252649c9a852"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/adityajn105/flickr30k\n",
            "License(s): CC0-1.0\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"archive (1).zip\" -d ./flickr30k"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7HF5jDh4M25",
        "outputId": "b845feb0-29a5-48fc-fa29-467a9c253cea"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open archive (1).zip, archive (1).zip.zip or archive (1).zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fkTP4xj4ayP",
        "outputId": "b21412a4-4f35-43ba-f231-e055048ed062"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kaggle.json  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwiZu1675JIF",
        "outputId": "4091110c-4665-456a-9a2b-a73113ab7b07"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.1.31)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.1)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.3)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.1.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.3.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d adityajn105/flickr30k"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUacvNW85MRA",
        "outputId": "f9dac2ca-8dcb-430a-8091-07f3d1230320"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/adityajn105/flickr30k\n",
            "License(s): CC0-1.0\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d hsankesara/flickr-image-dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyGlqovb5YGP",
        "outputId": "75f3684f-56c1-450a-8014-6dca69e20b48"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/hsankesara/flickr-image-dataset\n",
            "License(s): CC0-1.0\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://github.com/awsaf49/flickr-dataset/releases/download/v1.0/flickr30k_part00\"\n",
        "!wget \"https://github.com/awsaf49/flickr-dataset/releases/download/v1.0/flickr30k_part01\"\n",
        "!wget \"https://github.com/awsaf49/flickr-dataset/releases/download/v1.0/flickr30k_part02\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-g6bGW5m6Wjm",
        "outputId": "11ac9c64-292e-48b6-db4f-e02919245d7f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-24 09:49:12--  https://github.com/awsaf49/flickr-dataset/releases/download/v1.0/flickr30k_part00\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/753516996/b878d7ef-9eca-4c26-90d5-463215017d7a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250324%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250324T094913Z&X-Amz-Expires=300&X-Amz-Signature=0937b2219c427535aecf7ec884ff02a593877fb154df790dea3d7974fd425b83&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dflickr30k_part00&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-03-24 09:49:13--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/753516996/b878d7ef-9eca-4c26-90d5-463215017d7a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250324%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250324T094913Z&X-Amz-Expires=300&X-Amz-Signature=0937b2219c427535aecf7ec884ff02a593877fb154df790dea3d7974fd425b83&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dflickr30k_part00&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1460052528 (1.4G) [application/octet-stream]\n",
            "Saving to: ‘flickr30k_part00’\n",
            "\n",
            "flickr30k_part00      8%[>                   ] 111.56M  30.7MB/s    eta 45s    ^C\n",
            "--2025-03-24 09:49:17--  https://github.com/awsaf49/flickr-dataset/releases/download/v1.0/flickr30k_part01\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/753516996/bfe784ab-6ade-48cc-9e05-6f439ae72a03?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250324%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250324T094917Z&X-Amz-Expires=300&X-Amz-Signature=c55cc5437891ac2046a2b13d164df5a61af9c99397c7b30874958f2266319294&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dflickr30k_part01&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-03-24 09:49:17--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/753516996/bfe784ab-6ade-48cc-9e05-6f439ae72a03?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250324%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250324T094917Z&X-Amz-Expires=300&X-Amz-Signature=c55cc5437891ac2046a2b13d164df5a61af9c99397c7b30874958f2266319294&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dflickr30k_part01&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1460052528 (1.4G) [application/octet-stream]\n",
            "Saving to: ‘flickr30k_part01’\n",
            "\n",
            "flickr30k_part01    100%[===================>]   1.36G  72.5MB/s    in 20s     \n",
            "\n",
            "2025-03-24 09:49:37 (69.6 MB/s) - ‘flickr30k_part01’ saved [1460052528/1460052528]\n",
            "\n",
            "--2025-03-24 09:49:38--  https://github.com/awsaf49/flickr-dataset/releases/download/v1.0/flickr30k_part02\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/753516996/bd0da572-e714-4989-abd8-112530866485?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250324%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250324T094938Z&X-Amz-Expires=300&X-Amz-Signature=c1f7b5503cc35876b93501d86999c7fd9a2b4b511418de0b21d0396098ab8f81&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dflickr30k_part02&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-03-24 09:49:38--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/753516996/bd0da572-e714-4989-abd8-112530866485?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250324%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250324T094938Z&X-Amz-Expires=300&X-Amz-Signature=c1f7b5503cc35876b93501d86999c7fd9a2b4b511418de0b21d0396098ab8f81&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dflickr30k_part02&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1460052529 (1.4G) [application/octet-stream]\n",
            "Saving to: ‘flickr30k_part02’\n",
            "\n",
            "flickr30k_part02     25%[====>               ] 349.57M  86.3MB/s    eta 13s    ^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat flickr30k_part00 flickr30k_part01 flickr30k_part02 > flickr30k.zip"
      ],
      "metadata": {
        "id": "1zGgwnbv63Wk"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm flickr30k_part00 flickr30k_part01 flickr30k_part02"
      ],
      "metadata": {
        "id": "ATx8oNSS68Uu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q flickr30k.zip -d ./flickr30k\n",
        "!rm flickr30k.zip"
      ],
      "metadata": {
        "id": "F2HVJvQ07Imd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow matplotlib numpy pillow tqdm"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v47wS8WW7t_3",
        "outputId": "94f8b5e7-8542-41bb-a065-7da4788b4f0e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import pickle"
      ],
      "metadata": {
        "id": "g2ip3cix7waN"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_folder = \"./flickr30k/images/\"\n",
        "caption_file = \"./flickr30k/results.csv\""
      ],
      "metadata": {
        "id": "mnABLeJq73lG"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "caption_file = \"./flickr30k/captions.txt\"\n",
        "captions = pd.read_csv(caption_file)\n",
        "print(captions.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKefup54751e",
        "outputId": "068a13f7-308c-4a2b-9f7b-cc4c287e393e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            image                                            caption\n",
            "0  1000092795.jpg   Two young guys with shaggy hair look at their...\n",
            "1  1000092795.jpg   Two young , White males are outside near many...\n",
            "2  1000092795.jpg   Two men in green shirts are standing in a yard .\n",
            "3  1000092795.jpg       A man in a blue shirt standing in a garden .\n",
            "4  1000092795.jpg            Two friends enjoy time spent together .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "image_folder = \"./flickr30k/Images/\"\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    try:\n",
        "        img = load_img(image_path, target_size=(299, 299))\n",
        "        img = img_to_array(img)\n",
        "        img = np.expand_dims(img, axis=0)\n",
        "        img = img / 255.0\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading image {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Verify the folder structure\n",
        "print(\"Sample images from the directory:\")\n",
        "!ls ./flickr30k/Images | head\n",
        "\n",
        "# Testing with one image\n",
        "sample_image_path = os.path.join(image_folder, captions['image'][0].strip())\n",
        "processed_image = preprocess_image(sample_image_path)\n",
        "if processed_image is not None:\n",
        "    print(f\"Processed image shape: {processed_image.shape}\")\n",
        "else:\n",
        "    print(\"Image processing failed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MltJ-WAe9B46",
        "outputId": "16bb7e00-9acc-4177-80b9-38b639c61338"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample images from the directory:\n",
            "1000092795.jpg\n",
            "10002456.jpg\n",
            "1000268201.jpg\n",
            "1000344755.jpg\n",
            "1000366164.jpg\n",
            "1000523639.jpg\n",
            "1000919630.jpg\n",
            "10010052.jpg\n",
            "1001465944.jpg\n",
            "1001545525.jpg\n",
            "Processed image shape: (1, 299, 299, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "# Load the InceptionV3 model pre-trained on ImageNet\n",
        "try:\n",
        "    model = InceptionV3(weights=\"imagenet\")\n",
        "    model = Model(inputs=model.input, outputs=model.layers[-2].output)\n",
        "    print(\"Model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "\n",
        "def extract_features(image_path):\n",
        "    try:\n",
        "        img = preprocess_image(image_path)\n",
        "        if img is None:\n",
        "            return None\n",
        "        img = preprocess_input(img)\n",
        "        features = model.predict(img)\n",
        "        features = np.squeeze(features)\n",
        "        return features\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting features from {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Extract features from a sample image\n",
        "sample_features = extract_features(sample_image_path)\n",
        "if sample_features is not None:\n",
        "    print(f\"Feature vector shape: {sample_features.shape}\")\n",
        "else:\n",
        "    print(\"Feature extraction failed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "My4eAh4J9jHB",
        "outputId": "f6209172-54f7-4029-908a-f84fd2293a42"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
            "\u001b[1m96112376/96112376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Model loaded successfully.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10s/step\n",
            "Feature vector shape: (2048,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import pickle\n",
        "\n",
        "# Load captions\n",
        "captions_file = \"./flickr30k/captions.txt\"\n",
        "try:\n",
        "    captions = pd.read_csv(captions_file)\n",
        "    print(\"Captions loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading captions file: {e}\")\n",
        "\n",
        "# Rename columns if necessary\n",
        "if len(captions.columns) == 1:\n",
        "    captions = pd.read_csv(captions_file, names=[\"image\", \"caption\"])\n",
        "\n",
        "# Dictionary to store image features\n",
        "image_features = {}\n",
        "\n",
        "# Iterate over each image and caption pair with a progress bar\n",
        "with tqdm(total=len(captions), desc=\"Extracting Features\", unit=\"image\") as pbar:\n",
        "    for index, row in captions.iterrows():\n",
        "        try:\n",
        "            image_id = row[\"image\"].split(\".\")[0]\n",
        "            image_path = f\"./flickr30k/Images/{row['image']}\"\n",
        "\n",
        "            if image_id not in image_features:\n",
        "                features = extract_features(image_path)\n",
        "                if features is not None:\n",
        "                    image_features[image_id] = features\n",
        "            pbar.update(1)\n",
        "        except Exception as e:\n",
        "            tqdm.write(f\"Error processing {row['image']}: {e}\")\n",
        "\n",
        "# Save the features to a file\n",
        "try:\n",
        "    with open(\"image_features.pkl\", \"wb\") as f:\n",
        "        pickle.dump(image_features, f)\n",
        "    print(\"Image features saved successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving image features: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "28a9b3618ced49aea406929e2a86da46",
            "72974ed711224275a690e173c4cfa9a9",
            "aede58571f9e4fcb89bc7f43c271c1ef",
            "dc7f25d3a12141289a8ae8d3c4eb40ff",
            "07f3ad40201944beb06bd4282fc37a7a",
            "e420afe2a8c04871950861717ba3614d",
            "a15a0f557f464e6ab334d0bf01a2d144",
            "723a44e8207843cea4ea51b42e321d96",
            "b3d448b0761943a98c330e9bbe71a87f",
            "29690ac61a584327b0bb335ba9824102",
            "81277823aba3483aae5e8a83e9ef51c0"
          ]
        },
        "id": "RO6K4QL1-JrL",
        "outputId": "982f420b-cc81-4e0b-86b0-65b93c902d1a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Captions loaded successfully.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting Features:   0%|          | 0/158915 [00:00<?, ?image/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "28a9b3618ced49aea406929e2a86da46"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image features saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Set the path to the captions file\n",
        "caption_file = \"./flickr30k/captions.txt\"\n",
        "\n",
        "# Check if the file exists\n",
        "if os.path.exists(caption_file):\n",
        "    print(\"Captions file found.\")\n",
        "else:\n",
        "    print(\"Error: Captions file not found. Please check the file path.\")\n",
        "\n",
        "# Load the captions\n",
        "captions = pd.read_csv(caption_file, delimiter=\",\")\n",
        "captions.columns = [\"image_id\", \"comment\"]\n",
        "\n",
        "# Display the first few rows\n",
        "print(\"Sample Captions:\")\n",
        "print(captions.head())\n",
        "\n",
        "# Group captions by image_id\n",
        "image_captions = captions.groupby(\"image_id\")[\"comment\"].apply(list).reset_index()\n",
        "print(f\"\\nTotal unique images with captions: {len(image_captions)}\")\n",
        "print(image_captions.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwZBs4tDMp3Y",
        "outputId": "8358f5e0-2b2a-4d06-8e66-51337137f346"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Captions file found.\n",
            "Sample Captions:\n",
            "         image_id                                            comment\n",
            "0  1000092795.jpg   Two young guys with shaggy hair look at their...\n",
            "1  1000092795.jpg   Two young , White males are outside near many...\n",
            "2  1000092795.jpg   Two men in green shirts are standing in a yard .\n",
            "3  1000092795.jpg       A man in a blue shirt standing in a garden .\n",
            "4  1000092795.jpg            Two friends enjoy time spent together .\n",
            "\n",
            "Total unique images with captions: 31783\n",
            "         image_id                                            comment\n",
            "0  1000092795.jpg  [ Two young guys with shaggy hair look at thei...\n",
            "1    10002456.jpg  [ Several men in hard hats are operating a gia...\n",
            "2  1000268201.jpg  [ A child in a pink dress is climbing up a set...\n",
            "3  1000344755.jpg  [ Someone in a blue shirt and hat is standing ...\n",
            "4  1000366164.jpg  [ Two men , one in a gray shirt , one in a bla...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def preprocess_caption(caption):\n",
        "    if isinstance(caption, float):\n",
        "        return \"\"  # Return an empty string for NaN values\n",
        "    # Convert to lowercase\n",
        "    caption = caption.lower()\n",
        "    # Remove punctuation\n",
        "    caption = caption.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    # Add start and end tokens\n",
        "    caption = f\"<start> {caption} <end>\"\n",
        "    return caption\n",
        "\n",
        "# Apply the preprocessing function to each caption in the list\n",
        "image_captions[\"comment\"] = image_captions[\"comment\"].apply(lambda x: [preprocess_caption(c) for c in x] if isinstance(x, list) else [])\n",
        "\n",
        "# Display some preprocessed captions\n",
        "print(\"Sample Preprocessed Captions:\")\n",
        "print(image_captions.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwbNenvWOFs1",
        "outputId": "d2419611-6066-41fc-aa40-d9322cccb6ad"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Preprocessed Captions:\n",
            "         image_id                                            comment\n",
            "0  1000092795.jpg  [<start>  two young guys with shaggy hair look...\n",
            "1    10002456.jpg  [<start>  several men in hard hats are operati...\n",
            "2  1000268201.jpg  [<start>  a child in a pink dress is climbing ...\n",
            "3  1000344755.jpg  [<start>  someone in a blue shirt and hat is s...\n",
            "4  1000366164.jpg  [<start>  two men  one in a gray shirt  one in...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pickle\n",
        "\n",
        "# Combine all captions into one list for tokenization\n",
        "all_captions = []\n",
        "for captions_list in image_captions[\"comment\"]:\n",
        "    all_captions.extend(captions_list)\n",
        "\n",
        "# Initialize and fit the tokenizer\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token=\"<unk>\")\n",
        "tokenizer.fit_on_texts(all_captions)\n",
        "\n",
        "# Save the tokenizer\n",
        "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "# Convert captions to sequences\n",
        "image_captions[\"comment_seq\"] = image_captions[\"comment\"].apply(\n",
        "    lambda captions: [tokenizer.texts_to_sequences([c])[0] for c in captions]\n",
        ")\n",
        "\n",
        "# Display some tokenized captions\n",
        "print(\"Sample Tokenized Captions:\")\n",
        "print(image_captions.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FAod7cqOd2G",
        "outputId": "5d490f90-0bf3-4da6-c342-1a08a10f404e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Tokenized Captions:\n",
            "         image_id                                            comment  \\\n",
            "0  1000092795.jpg  [<start>  two young guys with shaggy hair look...   \n",
            "1    10002456.jpg  [<start>  several men in hard hats are operati...   \n",
            "2  1000268201.jpg  [<start>  a child in a pink dress is climbing ...   \n",
            "3  1000344755.jpg  [<start>  someone in a blue shirt and hat is s...   \n",
            "4  1000366164.jpg  [<start>  two men  one in a gray shirt  one in...   \n",
            "\n",
            "                                         comment_seq  \n",
            "0  [[4, 14, 21, 324, 12, 2105, 110, 186, 18, 63, ...  \n",
            "1  [[4, 119, 31, 5, 330, 274, 15, 1309, 2, 810, 3...  \n",
            "2  [[4, 2, 50, 5, 2, 87, 116, 10, 244, 49, 2, 359...  \n",
            "3  [[4, 277, 5, 2, 27, 23, 8, 64, 10, 33, 7, 2833...  \n",
            "4  [[4, 14, 31, 43, 5, 2, 120, 23, 43, 5, 2, 24, ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the maximum length of caption sequences\n",
        "max_length = max(len(seq) for caption_seqs in image_captions[\"comment_seq\"] for seq in caption_seqs)\n",
        "\n",
        "# Pad the sequences\n",
        "image_captions[\"comment_seq\"] = image_captions[\"comment_seq\"].apply(\n",
        "    lambda captions: [pad_sequences([seq], maxlen=max_length, padding=\"post\")[0] for seq in captions]\n",
        ")\n",
        "\n",
        "# Save the maximum length for later use\n",
        "with open(\"max_length.pkl\", \"wb\") as f:\n",
        "    pickle.dump(max_length, f)\n",
        "\n",
        "# Display some padded tokenized captions\n",
        "print(\"Sample Padded Tokenized Captions:\")\n",
        "print(image_captions.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kTyLToiOowR",
        "outputId": "e43eb4da-6c18-4b74-8098-cd780f1ccf44"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Padded Tokenized Captions:\n",
            "         image_id                                            comment  \\\n",
            "0  1000092795.jpg  [<start>  two young guys with shaggy hair look...   \n",
            "1    10002456.jpg  [<start>  several men in hard hats are operati...   \n",
            "2  1000268201.jpg  [<start>  a child in a pink dress is climbing ...   \n",
            "3  1000344755.jpg  [<start>  someone in a blue shirt and hat is s...   \n",
            "4  1000366164.jpg  [<start>  two men  one in a gray shirt  one in...   \n",
            "\n",
            "                                         comment_seq  \n",
            "0  [[4, 14, 21, 324, 12, 2105, 110, 186, 18, 63, ...  \n",
            "1  [[4, 119, 31, 5, 330, 274, 15, 1309, 2, 810, 3...  \n",
            "2  [[4, 2, 50, 5, 2, 87, 116, 10, 244, 49, 2, 359...  \n",
            "3  [[4, 277, 5, 2, 27, 23, 8, 64, 10, 33, 7, 2833...  \n",
            "4  [[4, 14, 31, 43, 5, 2, 120, 23, 43, 5, 2, 24, ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and validation sets (80-20 split)\n",
        "train_data, val_data = train_test_split(image_captions, test_size=0.2, random_state=42)\n",
        "\n",
        "# Save the split data for later use\n",
        "train_data.to_pickle(\"train_data.pkl\")\n",
        "val_data.to_pickle(\"val_data.pkl\")\n",
        "\n",
        "print(f\"Training samples: {len(train_data)}\")\n",
        "print(f\"Validation samples: {len(val_data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRC_Gj5jOuQS",
        "outputId": "e1fa013b-16c9-4d06-f8bb-0cacfc782a8d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 25426\n",
            "Validation samples: 6357\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, AdditiveAttention, Input, Dropout, Concatenate, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "embedding_dim = 256\n",
        "units = 512\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Encoder Model\n",
        "def build_encoder():\n",
        "    encoder_input = Input(shape=(2048,))\n",
        "    x = Dense(embedding_dim, activation=\"relu\")(encoder_input)\n",
        "    x = Dropout(0.5)(x)\n",
        "    # Add sequence dimension (batch_size, 1, embedding_dim)\n",
        "    x = Lambda(lambda tensor: tf.expand_dims(tensor, axis=1))(x)\n",
        "    return Model(encoder_input, x)\n",
        "\n",
        "# Decoder Model\n",
        "def build_decoder():\n",
        "    decoder_input = Input(shape=(None,))\n",
        "    features_input = Input(shape=(1, embedding_dim))  # Encoder output shape\n",
        "\n",
        "    embedding = Embedding(vocab_size, embedding_dim)(decoder_input)\n",
        "\n",
        "    # Compute attention (output shape: [batch_size, seq_length, embedding_dim])\n",
        "    attention = AdditiveAttention()([embedding, features_input])\n",
        "\n",
        "    # Concatenate attention context with embedding\n",
        "    combined = Concatenate(axis=-1)([attention, embedding])\n",
        "\n",
        "    lstm_output, state_h, state_c = LSTM(units, return_sequences=True, return_state=True)(combined)\n",
        "    output = Dense(vocab_size, activation=\"softmax\")(lstm_output)\n",
        "\n",
        "    return Model([decoder_input, features_input], output)\n",
        "\n",
        "# Build Models\n",
        "encoder = build_encoder()\n",
        "decoder = build_decoder()\n",
        "\n",
        "encoder.summary()\n",
        "decoder.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "id": "QAB7WDPwO17p",
        "outputId": "2952ae8b-57e5-470a-fcbf-34e67c84a47a"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_8\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_8\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_25 (\u001b[38;5;33mInputLayer\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │         \u001b[38;5;34m524,544\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lambda_2 (\u001b[38;5;33mLambda\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lambda_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m524,544\u001b[0m (2.00 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> (2.00 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m524,544\u001b[0m (2.00 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> (2.00 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_9\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_9\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_26            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_7 (\u001b[38;5;33mEmbedding\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │      \u001b[38;5;34m5,123,072\u001b[0m │ input_layer_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_27            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ additive_attention_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │            \u001b[38;5;34m256\u001b[0m │ embedding_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│ (\u001b[38;5;33mAdditiveAttention\u001b[0m)       │                        │                │ input_layer_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_3             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ additive_attention_1[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ embedding_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)               │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m),    │      \u001b[38;5;34m2,099,200\u001b[0m │ concatenate_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,    │                │                        │\n",
              "│                           │ \u001b[38;5;34m512\u001b[0m)]                  │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20012\u001b[0m)    │     \u001b[38;5;34m10,266,156\u001b[0m │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_26            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,123,072</span> │ input_layer_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_27            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ additive_attention_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ embedding_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AdditiveAttention</span>)       │                        │                │ input_layer_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_3             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ additive_attention_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ embedding_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)               │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>),    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,099,200</span> │ concatenate_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,    │                │                        │\n",
              "│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)]                  │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20012</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">10,266,156</span> │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m17,488,684\u001b[0m (66.71 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,488,684</span> (66.71 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m17,488,684\u001b[0m (66.71 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,488,684</span> (66.71 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load captions\n",
        "captions_file = \"./flickr30k/captions.txt\"\n",
        "try:\n",
        "    captions = pd.read_csv(captions_file)\n",
        "    print(\"Captions loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading captions file: {e}\")\n",
        "    # Try alternative format if standard CSV loading fails\n",
        "    captions = pd.read_csv(captions_file, delimiter=\"|\", names=[\"image\", \"caption\"])\n",
        "\n",
        "# Rename columns if necessary\n",
        "if len(captions.columns) == 1:\n",
        "    captions = pd.read_csv(captions_file, names=[\"image\", \"caption\"])\n",
        "\n",
        "# Load image features from the pickle file\n",
        "try:\n",
        "    with open(\"image_features.pkl\", \"rb\") as f:\n",
        "        features = pickle.load(f)\n",
        "    print(\"Image features loaded successfully.\")\n",
        "    # Verify feature shape\n",
        "    sample_feature = next(iter(features.values()))\n",
        "    print(f\"Image feature shape: {sample_feature.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading image features: {e}\")\n",
        "\n",
        "# Preprocess captions: convert to string and add start/end tokens\n",
        "captions[\"caption\"] = captions[\"caption\"].astype(str)\n",
        "captions[\"caption\"] = captions[\"caption\"].apply(lambda x: \"<start> \" + x + \" <end>\")\n",
        "captions = captions[captions[\"caption\"].str.strip() != \"<start>  <end>\"]\n",
        "print(f\"Number of valid captions after preprocessing: {len(captions)}\")\n",
        "\n",
        "# Tokenize captions\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=\"<unk>\")\n",
        "tokenizer.fit_on_texts(captions[\"caption\"].values)\n",
        "\n",
        "# Check special tokens\n",
        "print(f\"Special tokens: <start>: {tokenizer.word_index.get('<start>')}, <end>: {tokenizer.word_index.get('<end>')}\")\n",
        "print(f\"Total vocabulary size: {len(tokenizer.word_index)}\")\n",
        "\n",
        "# Save tokenizer for later use\n",
        "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "# Convert captions to sequences and pad them\n",
        "cap_seqs = tokenizer.texts_to_sequences(captions[\"caption\"].values)\n",
        "cap_seqs = tf.keras.preprocessing.sequence.pad_sequences(cap_seqs, padding=\"post\")\n",
        "\n",
        "print(\"Data preprocessing completed successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpHFmh0TRvB5",
        "outputId": "afa434e9-64ee-4cef-cbaf-8f5406b0338e"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Captions loaded successfully.\n",
            "Image features loaded successfully.\n",
            "Image feature shape: (2048,)\n",
            "Number of valid captions after preprocessing: 158915\n",
            "Special tokens: <start>: None, <end>: None\n",
            "Total vocabulary size: 18314\n",
            "Data preprocessing completed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Group captions by image\n",
        "captions_grouped = captions.groupby(\"image\")[\"caption\"].apply(list).reset_index()\n",
        "\n",
        "img_features_list = []\n",
        "cap_seqs_list = []\n",
        "\n",
        "# Verify the image naming convention\n",
        "sample_image = captions[\"image\"].iloc[0]\n",
        "print(f\"Sample image filename: {sample_image}\")\n",
        "\n",
        "for _, row in captions_grouped.iterrows():\n",
        "    # Extract image ID based on filename format (adjust split if needed)\n",
        "    img_id = row[\"image\"].split(\".\")[0]\n",
        "\n",
        "    if img_id in features:\n",
        "        for caption in row[\"caption\"]:\n",
        "            if caption:\n",
        "                seq = tokenizer.texts_to_sequences([caption])[0]\n",
        "                if len(seq) > 0:\n",
        "                    img_features_list.append(features[img_id])\n",
        "                    cap_seqs_list.append(seq)\n",
        "                else:\n",
        "                    print(f\"Empty sequence for caption: {caption}\")\n",
        "            else:\n",
        "                print(f\"Empty caption found for image: {img_id}\")\n",
        "    else:\n",
        "        print(f\"Image {img_id} not found in features.\")\n",
        "\n",
        "if len(cap_seqs_list) == 0:\n",
        "    raise ValueError(\"No valid caption sequences found. Check your preprocessing steps.\")\n",
        "\n",
        "print(f\"Total number of image-caption pairs: {len(img_features_list)}\")\n",
        "\n",
        "# Convert lists to arrays\n",
        "img_features_array = np.array(img_features_list)\n",
        "cap_seqs_array = tf.keras.preprocessing.sequence.pad_sequences(cap_seqs_list, padding=\"post\")\n",
        "\n",
        "print(f\"Image features array shape: {img_features_array.shape}\")\n",
        "print(f\"Caption sequences array shape: {cap_seqs_array.shape}\")\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((img_features_array, cap_seqs_array))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "print(\"Dataset prepared successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyHWh4PCUWTh",
        "outputId": "756cfbfb-b19f-455a-a0fd-f791d30a9399"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample image filename: 1000092795.jpg\n",
            "Total number of image-caption pairs: 158915\n",
            "Image features array shape: (158915, 2048)\n",
            "Caption sequences array shape: (158915, 80)\n",
            "Dataset prepared successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Embedding, GRU\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Encoder Model\n",
        "class CNN_Encoder(tf.keras.Model):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "        self.fc = Dense(embedding_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        return x\n",
        "\n",
        "# Decoder Model - Simplified implementation\n",
        "class RNN_Decoder(tf.keras.Model):\n",
        "    def __init__(self, embedding_dim, units, vocab_size):\n",
        "        super(RNN_Decoder, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "        # Define all layers explicitly\n",
        "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Use a standard GRU layer instead of one with return_sequences and return_state\n",
        "        self.gru = tf.keras.layers.GRU(\n",
        "            units,\n",
        "            return_state=False,\n",
        "            return_sequences=False\n",
        "        )\n",
        "\n",
        "        # Add a wrapper to manually handle state\n",
        "        self.gru_cell = tf.keras.layers.GRUCell(units)\n",
        "\n",
        "        # Output layer\n",
        "        self.fc = Dense(vocab_size)\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "        # x shape: (batch, 1)\n",
        "        x = self.embedding(x)  # (batch, 1, embedding_dim)\n",
        "\n",
        "        # Reshape for GRU cell\n",
        "        x = tf.squeeze(x, axis=1)  # Remove the time dimension for the cell\n",
        "\n",
        "        # Use the GRU cell which should be more compatible with graph mode\n",
        "        output, state = self.gru_cell(x, [hidden])\n",
        "\n",
        "        # Pass through the output layer\n",
        "        prediction = self.fc(output)  # (batch, vocab_size)\n",
        "\n",
        "        return prediction, state[0]\n",
        "\n",
        "# Define model hyperparameters\n",
        "embedding_dim = 256\n",
        "units = 512\n",
        "vocab_size = len(tokenizer.word_index) + 1  # +1 for padding token (0)\n",
        "print(f\"Using vocabulary size: {vocab_size}\")\n",
        "\n",
        "# Initialize models\n",
        "encoder = CNN_Encoder(embedding_dim)\n",
        "decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n",
        "\n",
        "# Define the hidden state initialization layer\n",
        "init_hidden_layer = tf.keras.layers.Dense(units, activation='relu')\n",
        "\n",
        "# Define optimizer and loss function\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# Setup checkpoint management\n",
        "checkpoint_path = \"./checkpoints/train\"\n",
        "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
        "                           decoder=decoder,\n",
        "                           optimizer=optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# Restore from checkpoint if available\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(f\"Restored from checkpoint: {ckpt_manager.latest_checkpoint}\")\n",
        "\n",
        "print(\"Model architecture created successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lurmrYPdUffq",
        "outputId": "27547446-2b02-4882-da58-6b728106d58e"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using vocabulary size: 18315\n",
            "Model architecture created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a Dense layer to initialize the decoder hidden state from encoder features\n",
        "init_hidden_layer = tf.keras.layers.Dense(units, activation='tanh')\n",
        "\n",
        "# Determine maximum caption length from the padded array\n",
        "MAX_LENGTH = cap_seqs_array.shape[1]\n",
        "print(f\"Maximum caption length: {MAX_LENGTH}\")\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "def train_step_eager(img_tensor, target):\n",
        "    loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Pass image through encoder to get features\n",
        "        features = encoder(img_tensor)  # shape: (batch, embedding_dim)\n",
        "        # Initialize decoder hidden state using a dense layer\n",
        "        hidden = init_hidden_layer(features)  # shape: (batch, units)\n",
        "        # Initialize decoder input with <start> token\n",
        "        start_token = tokenizer.word_index.get(\"<start>\", 1)\n",
        "        dec_input = tf.expand_dims([start_token] * target.shape[0], 1)  # (batch, 1)\n",
        "\n",
        "        batch_loss = 0\n",
        "        # Loop through each time step in the target sequence (starting from index 1)\n",
        "        for i in range(1, target.shape[1]):\n",
        "            predictions, hidden = decoder(dec_input, hidden)\n",
        "            loss_t = loss_function(target[:, i], predictions)\n",
        "            batch_loss += loss_t\n",
        "            # Teacher forcing: use true token as next input\n",
        "            dec_input = tf.expand_dims(target[:, i], 1)\n",
        "\n",
        "        loss = batch_loss / tf.cast(target.shape[1] - 1, tf.float32)\n",
        "\n",
        "    trainable_vars = encoder.trainable_variables + decoder.trainable_variables + init_hidden_layer.trainable_variables\n",
        "    gradients = tape.gradient(loss, trainable_vars)\n",
        "    optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "    return loss, loss\n",
        "\n",
        "# Training loop using eager execution for testing\n",
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss = 0\n",
        "    steps = 0\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
        "        batch_loss, t_loss = train_step_eager(img_tensor, target)\n",
        "        total_loss += t_loss\n",
        "        steps += 1\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            print(f\"Epoch {epoch+1} Batch {batch} Loss {batch_loss:.4f}\")\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        checkpoint_path = ckpt_manager.save()\n",
        "        print(f\"Checkpoint saved at: {checkpoint_path}\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Loss {total_loss/steps:.4f}\\n\")\n",
        "\n",
        "print(\"Training completed successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "7s9UDlsgYRNm",
        "outputId": "a77047f3-58db-4b12-ae6e-68f3363704a3"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum caption length: 80\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Unknown variable: <Variable path=dense_38/kernel, shape=(256, 512), dtype=float32, value=[[ 0.07858204  0.03212717 -0.00867839 ... -0.01703987 -0.01866675\n   0.05151369]\n [-0.0449603  -0.06677287  0.00250459 ... -0.01897706 -0.05688644\n   0.04476895]\n [-0.03124569  0.0777277  -0.01285005 ...  0.04813465  0.0081782\n   0.01033258]\n ...\n [ 0.0781777   0.08347898  0.07753735 ... -0.04221414 -0.03748796\n  -0.06637661]\n [ 0.0669545  -0.01057252  0.0491598  ... -0.04097704  0.01215129\n   0.03495881]\n [ 0.07315575  0.00582822  0.04268949 ... -0.05920309 -0.00462852\n  -0.03985115]]>. This optimizer can only be called for the variables it was originally built with. When working with a new set of variables, you should recreate a new optimizer instance.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-100-7efe8f2ad8d5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mbatch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mt_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-100-7efe8f2ad8d5>\u001b[0m in \u001b[0;36mtrain_step_eager\u001b[0;34m(img_tensor, target)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mtrainable_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minit_hidden_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0;31m# Return iterations for compat with tf.keras.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[1;32m    422\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_variables_are_known\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py\u001b[0m in \u001b[0;36m_check_variables_are_known\u001b[0;34m(self, variables)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_var_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainable_variables_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    330\u001b[0m                     \u001b[0;34mf\"Unknown variable: {v}. This optimizer can only \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                     \u001b[0;34m\"be called for the variables it was originally built with. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unknown variable: <Variable path=dense_38/kernel, shape=(256, 512), dtype=float32, value=[[ 0.07858204  0.03212717 -0.00867839 ... -0.01703987 -0.01866675\n   0.05151369]\n [-0.0449603  -0.06677287  0.00250459 ... -0.01897706 -0.05688644\n   0.04476895]\n [-0.03124569  0.0777277  -0.01285005 ...  0.04813465  0.0081782\n   0.01033258]\n ...\n [ 0.0781777   0.08347898  0.07753735 ... -0.04221414 -0.03748796\n  -0.06637661]\n [ 0.0669545  -0.01057252  0.0491598  ... -0.04097704  0.01215129\n   0.03495881]\n [ 0.07315575  0.00582822  0.04268949 ... -0.05920309 -0.00462852\n  -0.03985115]]>. This optimizer can only be called for the variables it was originally built with. When working with a new set of variables, you should recreate a new optimizer instance."
          ]
        }
      ]
    }
  ]
}